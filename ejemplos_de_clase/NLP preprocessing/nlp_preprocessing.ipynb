{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</center><img src=\"https://www.teachingenglish.org.uk/sites/teacheng/files/styles/large/public/images/revisiting_text_iStock_000015756375XSmall%20%281%29_3.jpg?itok=YSNa9Dvz\" width=600></center>\n",
    "<h1 align=\"center\">Natural Language Processing</h1>\n",
    "<h2 align=\"center\"> <font color='gray'>Pre-processing</font></h2>\n",
    "\n",
    "### <font color='289C4E'>Tabla de contenidos<font> \n",
    "- [Convertir un string a mayúscula o minúscula](#lower)\n",
    "- [Remover caracteres especiales](#remove)\n",
    "- [Stop word removal](#stop)\n",
    "- [Stemming](#stemming)\n",
    "- [Lematización](#lemmatisation)\n",
    "- [POS tagging](#pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>Convertir un string a mayúscula o minúscula<font> <a class=\"anchor\" id='lower'></a>\n",
    "  \n",
    "Convertir toda palabra a minúsculas o mayúsculas. Está hecho porque al vectorizar los datos tendremos 2 dimensiones diferentes para la misma palabra. Por ejemplo, \"SUPERIOR\" y \"superior\" tendrán una dimensión diferente. Si convertimos a menor tendremos 1 dimensión por cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep():\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper can be written in different ways Upper, UPPER, UPPer, UPpeR but this can be converted to same form\n",
      "----------------------------------------------------------------------------------------------------\n",
      "upper can be written in different ways upper, upper, upper, upper but this can be converted to same form\n",
      "----------------------------------------------------------------------------------------------------\n",
      "UPPER CAN BE WRITTEN IN DIFFERENT WAYS UPPER, UPPER, UPPER, UPPER BUT THIS CAN BE CONVERTED TO SAME FORM\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Upper can be written in different ways Upper, UPPER, UPPer, UPpeR but this can be converted to same form'\n",
    "print(sentence)\n",
    "sep()\n",
    "print(sentence.lower())\n",
    "sep()\n",
    "print(sentence.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>Remover caracteres especiales<font> <a class=\"anchor\" id='remove'></a>\n",
    "\n",
    "El texto sin formato/crudo que tengamos tendrá mucho ruido, como puntuación, caracteres especiales y espacios en blanco adicionales. En algunos casos, no agrega significado al texto/oración. Por lo tanto, se pueden eliminar utilizando la popular biblioteca de Python llamada regex (re) o una string function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', '&is', '[an]', 'example?', '{of}', \"'string'.\", 'with.?', 'Punctuation!!!!']\n"
     ]
    }
   ],
   "source": [
    "# import WhitespaceTokenizer() method from nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "     \n",
    "# Create a reference variable for Class WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "     \n",
    "# Create a string input\n",
    "text = \"This &is [an] example? {of} 'string'. with.? Punctuation!!!!\"\n",
    "     \n",
    "# Use tokenize method\n",
    "tokenized_text = tk.tokenize(text)\n",
    "     \n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with Punctuation \n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "text = \"This &is [an] example? {of} 'string'. with.? Punctuation!!!!\"\n",
    "print(re.sub('[^A-Za-z0-9]+', ' ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'of', 'string', 'with', 'Punctuation']\n"
     ]
    }
   ],
   "source": [
    "# Use tokenize method\n",
    "\n",
    "text = \"This &is [an] example? {of} 'string'. with.? Punctuation!!!!\"\n",
    "text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "\n",
    "tokenized_text = tk.tokenize(text)\n",
    "     \n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with Punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "input_str = \"This &is [an] example? {of} string. with.? Punctuation!!!!\" # Sample string\n",
    "result = result = input_str.translate(str.maketrans('', '', string.punctuation)) #removes [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>Stop word removal<font> <a class=\"anchor\" id='stop'></a>\n",
    "\n",
    "El texto puede contener stop words como ‘the’, ‘is’, ‘are’. Las stop words se pueden filtrar del texto que se va a procesar. No existe una lista universal de stop words en la literatura de NLP, sin embargo, el módulo `nltk` contiene una lista de stop words.\n",
    "\n",
    "La remoción de stop words depende de la tarea. Por ejemplo, si tiene una tarea de clasificación de texto o análisis de sentimientos, debe eliminar las stop words, ya que no proporcionan ninguna información para modelar, pero si tiene una tarea de traducción de idiomas, las stop words son útiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "en_stops = set(stopwords.words('english'))\n",
    "en_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence going remove predefined stopwords , ready\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In this sentence we are going to remove predefined stopwords, are you ready\"\n",
    "sentence_aft_SW_removal = \" \".join([word for word in word_tokenize(sentence.lower()) if word not in en_stops])\n",
    "#sentense_aft_SW_removel = \" \".join([word for word in sentense.lower().split() if word not in en_stops]) #can try this as well\n",
    "print(sentence_aft_SW_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>Stemming<font> <a class=\"anchor\" id='stemming'></a>\n",
    "\n",
    "Stemming es el proceso de reducir la inflexión de las palabras a sus formas de raíz, como mapear un grupo de palabras con la misma raíz, incluso si la raíz en sí no es una palabra válida en el idioma. Stemming por lo general recorta la palabra usando un conjunto de reglas, por ejemplo, `plays`, `playing` y `played` se recorta a `play` eliminando el sufijo \"s\", \"ing\" y \"ed\".\n",
    "\n",
    "Hay principalmente dos errores en stemming: **over-stemming y under-stemming.**\n",
    "\n",
    "Over-stemming ocurre cuando dos palabras provienen de la misma raíz y tienen diferentes raíces. Over-stemming también se puede considerar como **falsos positivos**.\n",
    "\n",
    "Under-stemming ocurre cuando dos palabras provienen de la misma raíz que no tienen raíces diferentes. Under-stemming se puede interpretar como **falsos negativos**.\n",
    "\n",
    "Más información sobre los diferentes tipos de stemming [aquí](https://www.geeksforgeeks.org/introduction-to-stemming/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'read', 'read', 'machin']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "Porter = PorterStemmer()\n",
    "list_of_words = ['played', 'playing', 'plays', 'reading', 'read', 'machine']\n",
    "post_stemming = [Porter.stem(word) for word in list_of_words]\n",
    "print(post_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>Lematización<font> <a class=\"anchor\" id='lemmatisation'></a>\n",
    "\n",
    "La diferencia entre stemming y la lematización es que **la lematización considera el contexto** y convierte la palabra a su forma básica significativa, mientras que la derivación simplemente elimina los últimos caracteres, lo que a menudo conduce a significados incorrectos y errores ortográficos.\n",
    "\n",
    "Por ejemplo, la lematización identificaría correctamente la forma básica de `caring` to `care`, mientras que stemming cortaría la parte \"ing\" y la convertiría en `car`. \n",
    "\n",
    "Lea más sobre diferentes lematizaciones [aquí](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caring', 'machine', 'play', 'playing', 'happening']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "list_of_words = ['caring', 'machine', 'plays','playing','happening']\n",
    "post_lemma = [lemma.lemmatize(word) for word in list_of_words]\n",
    "print(post_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que no hizo un buen trabajo. Porque la mayoría de las palabras siguen siendo las mismas. Esto puede corregirse si proporcionamos la correcta tag de **‘part-of-speech’ (POS tag)** como segundo argumento para `lemmatize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['care', 'machine', 'play', 'play', 'happen']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "list_of_words = ['caring', 'machine', 'plays','playing','happening']\n",
    "post_lemma = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in list_of_words]\n",
    "print(post_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='289C4E'>POS Tagging<font> <a class=\"anchor\" id='pos'></a>\n",
    "\n",
    "POS tagging es el proceso de etiquetar cada palabra en una oración con su parte apropiada del discurso. Ya sabemos que las partes del discurso incluyen sustantivos, verbos, adverbios, adjetivos, pronombres, conjunciones y sus subcategorías.\n",
    "\n",
    "La mayor parte del POS tagging cae dentro de Rule based POS tagging, Stochastic POS tagging y Transformation based tagging. \n",
    "\n",
    "En este ejemplo a continuación, tenemos un texto con 4 oraciones. Tenga en cuenta la palabra `best` en las 4 oraciones. La misma palabra \"`best` se usa de manera diferente en las cuatro oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The goal was to best the competition.', 'His latest song was a personal best.', 'Hence, he received the best song of the year award.', 'He played best after a couple of martinis.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag_sents\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    " \n",
    "text = \"\"\"The goal was to best the competition. His latest song was a personal best.\n",
    "        Hence, he received the best song of the year award. He played best after a couple of martinis.\"\"\"\n",
    " \n",
    "text_sentence_tokens = sent_tokenize(text)\n",
    "print (text_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'goal', 'was', 'to', 'best', 'the', 'competition', '.'], ['His', 'latest', 'song', 'was', 'a', 'personal', 'best', '.'], ['Hence', ',', 'he', 'received', 'the', 'best', 'song', 'of', 'the', 'year', 'award', '.'], ['He', 'played', 'best', 'after', 'a', 'couple', 'of', 'martinis', '.']]\n"
     ]
    }
   ],
   "source": [
    "text_word_tokens = []\n",
    "for sentence_token in text_sentence_tokens:\n",
    "    text_word_tokens.append(word_tokenize(sentence_token))\n",
    "print (text_word_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DT'), ('goal', 'NN'), ('was', 'VBD'), ('to', 'TO'), ('best', 'VB'), ('the', 'DT'), ('competition', 'NN'), ('.', '.')], [('His', 'PRP$'), ('latest', 'JJS'), ('song', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('personal', 'JJ'), ('best', 'NN'), ('.', '.')], [('Hence', 'RB'), (',', ','), ('he', 'PRP'), ('received', 'VBD'), ('the', 'DT'), ('best', 'JJS'), ('song', 'NN'), ('of', 'IN'), ('the', 'DT'), ('year', 'NN'), ('award', 'NN'), ('.', '.')], [('He', 'PRP'), ('played', 'VBD'), ('best', 'RB'), ('after', 'IN'), ('a', 'DT'), ('couple', 'NN'), ('of', 'IN'), ('martinis', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "text_tagged = pos_tag_sents(text_word_tokens)\n",
    "print (text_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "Estos son algunos de los pasos de pre-procesamiento de texto en NLP. Se pueden usar varias bibliotecas de Python como `nltk`, `spaCy` y `TextBlob`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
